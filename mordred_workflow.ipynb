{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mordred\n",
    "from mordred import Calculator, descriptors\n",
    "\n",
    "# Python standard library\n",
    "import os, sys\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Utils\n",
    "sys.path.append(\".\")\n",
    "from utils import classification_metrics as cmetrics\n",
    "from utils import finger_prints as fp\n",
    "from utils import classification_workflow_functions as cwf\n",
    "\n",
    "import logging \n",
    "logging.basicConfig(format='%(message)s')\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.ERROR)\n",
    "\n",
    "from dask.distributed import Client\n",
    "try:\n",
    "    client.shutdown()\n",
    "except NameError:\n",
    "    log.info(\"No client already active\")\n",
    "\n",
    "client = Client(dashboard_address=\":8855\")\n",
    "log.info(\"Dask clinet on localhost:8855\")\n",
    "\n",
    "random_seed = 10459\n",
    "np.random.seed = random_seed\n",
    "np.random.RandomState(random_seed)\n",
    "log.info(f\"Random seed fixed as {random_seed} current working dir {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results_mordred\", exist_ok=True)\n",
    "os.chdir(\"results_mordred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/ccs-98.csv\", sep=\";\")\n",
    "data.columns = [\"_\".join(ent.lower().strip().split(\" \")) for ent in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = data[\"smiles\"]\n",
    "log.info(\"SMILES: {}\".format(smiles.head()))\n",
    "\n",
    "names = data[\"label\"]\n",
    "log.info(\"Names: {}\".format(names.head()))\n",
    "\n",
    "number_of_n_atoms = data[\"n_nitrogen\"].astype(\"int\")\n",
    "log.info(\"Number of N atoms: {}\".format(number_of_n_atoms.head()))\n",
    "\n",
    "amines_mass_mr = data[\"molecular_mass\"].astype(\"float64\")\n",
    "pd.to_numeric(amines_mass_mr, errors=\"coerce\")\n",
    "log.info(\"Amines mass Mr: {}\".format(amines_mass_mr.head()))\n",
    "\n",
    "molco2_moln = data[\"capacity_molco2_molamime\"]\n",
    "pd.to_numeric(molco2_moln, errors=\"coerce\")\n",
    "log.info(\"molCO2_molN: {}\".format(molco2_moln.head()))\n",
    "\n",
    "initial_rates = data[\"rate_molco2_molamime_min\"]\n",
    "pd.to_numeric(initial_rates, errors=\"coerce\")\n",
    "log.info(\"initial_rates: {}\".format(initial_rates.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Target Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = molco2_moln\n",
    "target_name = \"capacity (molCO2 / molN)\"\n",
    "target_key = \"capacity_molco2_molamime\"\n",
    "units = \"molco2_moln\"\n",
    "threshold_for_catagorical = 50.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Mordred Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = Calculator(descriptors, ignore_3D=False)\n",
    "molecule_list = [cwf.get_mol_from_smiles(s) for s in smiles]\n",
    "features_df = calc.pandas(molecule_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.dropna(inplace=True, thresh=int(0.9*len(features_df.index)))\n",
    "threshold = 0.5\n",
    "features_df.drop(features_df.std()[features_df.std() < threshold].index.values, axis=1)\n",
    "features_df.columns = [ent.strip() for ent in features_df.columns]\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(features_df)\n",
    "feature_types = \"no_catagorical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_csv(\"mordred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate significant featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasonable_predicted_properties, significant_fearures = cwf.find_correlating_features(features_df, targets, thresh=0.5, \n",
    "                                                                                      plot=False, corr_method=\"spearman\", \n",
    "                                                                                      sig_metric=\"spearman\", process_non_numeric=True, \n",
    "                                                                                      sig_level=0.05, significance=True, n_sample=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"{} {}\".format(reasonable_predicted_properties, len(reasonable_predicted_properties)))\n",
    "log.info(\"{} {}\".format(significant_fearures, len(significant_fearures)))\n",
    "use_significant = True\n",
    "use_reasonable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df = pd.DataFrame()\n",
    "\n",
    "if use_significant is True:\n",
    "    for k in significant_fearures:\n",
    "        feats_df[k] = features_df[k]\n",
    "        \n",
    "elif use_reasonable is True:\n",
    "    for k in reasonable_predicted_properties:\n",
    "        feats_df[k] = features_df[k]\n",
    "        \n",
    "feats_df.to_csv(\"mordred-features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = []\n",
    "for ith, s in enumerate(smiles):\n",
    "    n_primary, n_secondary, n_tertiary, n_aromaticsp2 = cwf.count_amine_types(s, show=False)\n",
    "    counts.append([n_primary, n_secondary, n_tertiary, n_aromaticsp2])\n",
    "    log.debug(\"\\n{}; number of primary: {} number of secondary: {} number of tertiary: {} number of aromatic sp2 nitrogen atoms: {}\\nsmiles {}\". format(ith, n_primary, n_secondary, n_tertiary, n_aromaticsp2, s))\n",
    "\n",
    "df = pd.DataFrame(data=counts, columns=[\"primary_amine_counts\",\"secondary_amine_counts\", \"tertiary_amine_counts\", \"aromatic_sp2_n\" ])\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if target_name == \"initial_rate\":\n",
    "    log.info(\"Initial rate class\")\n",
    "    mean = np.mean(initial_rates)\n",
    "    stdev = np.std(initial_rates)\n",
    "    class_thresh = mean + stdev\n",
    "    log.info(\"mean {} standard deviation {} class threshold {}\".format(mean, stdev, class_thresh))\n",
    "    classes = []\n",
    "    for i in initial_rates:\n",
    "        if i < class_thresh:\n",
    "            classes.append(0)\n",
    "        else:\n",
    "            classes.append(1)\n",
    "    log.info(\"Number of classes: {}  Number of class 1: {} number of class 0: {}\".format(len(classes), len([x for x in classes if x == 1]), len([x for x in classes if x == 0])))\n",
    "    class_targets_df = pd.DataFrame(np.array([classes]).T, columns=[\"classes\"])\n",
    "    features_and_classes_df = features_df.copy()\n",
    "    features_and_classes_df[\"classes\"] = classes\n",
    "    \n",
    "else:\n",
    "    log.info(\"Capture capacity class\")\n",
    "    classes = cwf.capacity_classes(df[\"primary_amine_counts\"].values, df[\"secondary_amine_counts\"].values, df[\"tertiary_amine_counts\"].values, df[\"aromatic_sp2_n\"].values, targets,\n",
    "                         units=units, number_N_atoms=number_of_n_atoms, amines_mr=amines_mass_mr)\n",
    "    log.info(classes)\n",
    "    log.info(\"Number of classes: {}  Number of class 1: {} number of class 0: {}\".format(len(classes), len([x for x in classes if x == 1]), len([x for x in classes if x == 0])))\n",
    "\n",
    "    class_targets_df = pd.DataFrame(np.array([classes]).T, columns=[\"classes\"])\n",
    "    features_and_classes_df = features_df.copy()\n",
    "    features_and_classes_df[\"classes\"] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_for_catagorical = 50.0\n",
    "log.info(\"Threshold for a catagorical feature is any feature where more than {}% of the point have have a value or all vlaues are separated by the same step size\".format(threshold_for_catagorical))\n",
    "catagorical_name = []\n",
    "catagorical_indx = []\n",
    "for ith, column in enumerate(feats_df.columns):\n",
    "    if column != \"training\":\n",
    "        vals = sorted(set(feats_df[column].values))\n",
    "        steps = [elt - eltp1 for elt, eltp1 in zip(vals, vals[1:])]\n",
    "        log.debug(f\"\\n{ith} {column}\\n{steps}\\n\")\n",
    "        percent = []\n",
    "        for step in steps:\n",
    "            percent.append(len([elt for elt in steps if elt == step])/len(steps) * 100.0)\n",
    "        log.debug(f\"percentage {ith} {column}: {percent}\\n\")\n",
    "\n",
    "        if any(elt >= threshold_for_catagorical for elt in percent):\n",
    "            log.info(\"More than {} point have the same value for {} {}\".format(threshold_for_catagorical, ith, column))\n",
    "            catagorical_indx.append(ith)\n",
    "            catagorical_name.append(column)\n",
    "        elif all(elt == steps[0] for elt in steps):\n",
    "            log.info(\"Same separating step size for all features in {} {}\".format(ith, column))\n",
    "\n",
    "log.info(catagorical_indx)\n",
    "log.info(catagorical_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = feats_df.columns\n",
    "mordred_features_df = pd.DataFrame(data=np.array([[i+1 for i in range(len(feats))], feats]).T, columns=[\"Index\", \"Feature\"])\n",
    "with open(\"modred_{}_feature.tex\".format(len(feats)), \"w\") as fout:\n",
    "    mordred_features_df.to_latex(fout, float_format=\"{:0.2f}\".format, position=\"H\", longtable=True, caption=\"Feature selected from Spearman correlation coefficient (\\textgreater 0.4) and two tail p test at 95\\%\", label=\"tbl:fingerprint_{}_features\".format(len(feats)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df_bkup = feats_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inx, f in enumerate(feats):\n",
    "    log.info(f\"-----\\nIndex: {inx}\\n{feats_df[f].values}\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = \"some_catagorical\"\n",
    "# NOTE: USER DEFINES THE LIST BELOW!!!!!!\n",
    "catagorical_indxs = [18, 19, 20, 21, 22, 28]\n",
    "# [0, 1, 2, 5, 6, 7, 8, 9, 10,14, 15, 20, 31, 32, 33, 34]\n",
    "# 0.6 [0, 1, 2, 3, 4]\n",
    "# 0.5 [0, 1, 2, 5, 6, 7, 8, 9, 10,14, 15, 20, 31, 32, 33, 34]\n",
    "# 0.4 [0, 1, 2, 25, 26, 27, 29, 30, 31, 32, 39, 40, 41, 47, 72, 73, 74, 75]\n",
    "# 0.4 old [0, 1, 2, 21, 22, 23, 25, 26, 35, 36, 37, 41, 43, 68, 69, 70, 71]\n",
    "feature_columns = feats_df.columns\n",
    "\n",
    "# Backup\n",
    "backup_feats_df = feats_df.copy()\n",
    "\n",
    "# None catagorical only scale the data as numbers\n",
    "if feature_types == \"no_catagorical\":\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    feats_df = mm_scaler.fit_transform(feats_df)\n",
    "    log.info(pd.DataFrame(feats_df, columns=feature_columns))\n",
    "    feats_df = pd.DataFrame(feats_df, columns=feature_columns)\n",
    "    \n",
    "# Some catagorical - Need to provide the indexes\n",
    "elif feature_types == \"some_catagorical\":\n",
    "    numeric_features = [feature_columns[i] for i in range(len(feature_columns)) if i not in catagorical_indxs]\n",
    "    numerical_transformer = MinMaxScaler()\n",
    "    categorical_features = [feature_columns[i] for i in range(len(feature_columns)) if i in catagorical_indxs]\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    if any(ent in categorical_features for ent in numeric_features):\n",
    "        log.warning(\"WARNING - numeric and catagorical feature specififed overlap\")\n",
    "        log.info(numeric_features)\n",
    "        log.info(categorical_features)\n",
    "    else:\n",
    "        log.info(\"Numerical features:\\n{} {}\".format(numeric_features, len(numeric_features)))\n",
    "        log.info(\"Catagorical features:\\n{} {}\".format(categorical_features, len(catagorical_indxs)))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numerical\", numerical_transformer, numeric_features),\n",
    "        ('catagorical', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    feats_df = preprocessor.fit_transform(feats_df)\n",
    "    feature_names = cwf.get_feature_names_from_column_transformers(preprocessor)\n",
    "    catagorical_indxs = [i for i in range(len(numeric_features), len(feature_names))]\n",
    "    log.info(feature_names)\n",
    "\n",
    "    log.info(type(feats_df))\n",
    "    try:\n",
    "        log.info(pd.DataFrame(feats_df, columns=feature_names))\n",
    "        feats_df = pd.DataFrame(feats_df, columns=feature_names)\n",
    "    except ValueError:\n",
    "        log.info(pd.DataFrame(feats_df.to_array(), columns=feature_names))\n",
    "        feats_df = pd.DataFrame(feats_df.to_array(), columns=feature_names)\n",
    "    log.info(\"catagorical indexes {}\".format(catagorical_indxs))\n",
    "    log.info(\"Catagorical features start on column name {} and end on {}\".format(feats_df.columns[catagorical_indxs[0]], feats_df.columns[catagorical_indxs[-1]]))\n",
    "    \n",
    "# All catagorical\n",
    "elif feature_types == \"catagorical\":\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    feats_df = categorical_transformer.fit_transform(feats_df).toarray()\n",
    "    feature_names = [categorical_transformer.get_feature_names(feature_columns)]\n",
    "    feats_df = pd.DataFrame(feats_df, columns=feature_names)\n",
    "    log.info(feats_df)\n",
    "\n",
    "# No scaling or other encoding\n",
    "else:\n",
    "    log.info(\"No scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_indexes = [ith for ith in range(0, catagorical_indxs[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df.to_csv(\"mordred_scaled_onehotencode_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df = pd.DataFrame(data=classes, columns=[\"classes\"])\n",
    "log.info(\"Number in class 0: {}\\nNumber in class 1: {}\\nNumber of examples: {}\".format(\n",
    "    len([ith for ith in classes if ith == 0]), \n",
    "    len([ith for ith in classes if ith == 1]), len(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfold_clf_names = [\"AdaBoost\",\"Logistic Regression\", \"Gaussian Process\"]\n",
    "\n",
    "kfold_classifiers = [\n",
    "    AdaBoostClassifier(random_state=random_seed),\n",
    "    LogisticRegression(random_state=random_seed, n_jobs=-1, solver=\"lbfgs\"),\n",
    "    GaussianProcessClassifier(random_state=random_seed, n_jobs=-1)\n",
    "]\n",
    "\n",
    "kfold_classifier_parameters = {\n",
    "    \"AdaBoost\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "    \"Logistic Regression\": {\"penalty\":[\"l2\", \"none\"], \"C\": [0.05, 0.1, 0.25, 0.5, 1.0, 1.25]},\n",
    "    \"Gaussian Process\": {\"kernel\":[1.0 * Matern(length_scale=1.0, nu=1.5), 1.0 * Matern(length_scale=1.0, nu=2.5), 1.0 * RBF(1.0),  1.0 * RBF(1.0) + WhiteKernel(noise_level=0.5)]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ith, ent in enumerate(feats_df.isnull().sum().values):\n",
    "    if ent != 0:\n",
    "        log.info(f\"Row {ith} is not free of nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cwf.kfold_test_imbalenced_classifiers_with_optimization(feats_df, classes_df, kfold_classifiers, kfold_classifier_parameters, \n",
    "                                                        overwrite=True, scale=False, cv=10, n_repeats=5, smiles=smiles, names=names,\n",
    "                                                        random_seed=random_seed, clf_names=kfold_clf_names, class_labels=(0,1),\n",
    "                                                        smote=True, smote_catagorical_indexes=catagorical_indxs, \n",
    "                                                        smote_continuous_indexes=continous_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_clf_names = [\"AdaBoost\",\"Logistic Regression\", \"Gaussian Process\"]\n",
    "directory_names = cwf.directory_names_from_classfier_names(kfold_clf_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_plt = []\n",
    "\n",
    "for ith, dirname in enumerate(directory_names):\n",
    "    log.info(\"\\n{}\\n-------------\\n\".format(dirname))\n",
    "    data = cwf.build_data_from_directory(dirname, max_folds=5)\n",
    "    \n",
    "    log.debug(\"Last value in the data frame: {}\".format(data[165:166]))\n",
    "    \n",
    "    probs = data[[\"prob0\", \"prob1\"]].to_numpy()\n",
    "    log.debug(\"Probablities for a few examples {}\".format(probs[0:3,0:2]))\n",
    "    \n",
    "    cm = cmetrics.get_confusion_matrix(data, predicted_column_name=\"prediction\", known_column_name=\"known\", return_dict=False)\n",
    "    log.debug(\"Confusion matrix for {}\\n{}\".format(dirname, cm))\n",
    "    \n",
    "    plt_name = \"capacity_{}_mordred.png\".format(\"_\".join([ent.lower() for ent in dirname.split()]))\n",
    "    files_plt.append(plt_name)\n",
    "    log.info(\"Saving plot to {}\\n{}\".format(plt_name, files_plt))\n",
    "    metrics = cmetrics.calculate_confusion_based_metrics(df=data, probabilities=probs, col_map=\"hsv\", positive_label=1, \n",
    "                                                         plt_filename=plt_name, all_classes=False, get_roc_curve=True, \n",
    "                                                         get_pr_curve=False, annotate=True, vmin=0, vmax=85,\n",
    "                                                         title=\"{}\".format(kfold_clf_names[ith]))\n",
    "    #log.info(\"{}\".format(\"\\n\".join([\"{}: {}\".format(k, v) for k, v in metrics.items()])))\n",
    "    \n",
    "    metrics_for_paper = {\n",
    "    \"accuracy\": metrics[\"accuracy\"],\n",
    "    \"sensitivity\": metrics[\"tpr\"],\n",
    "    \"specificity\": metrics[\"tnr\"],\n",
    "    \"mcc\": metrics[\"matthews_correlation_coefficient\"],\n",
    "    \"precision\": metrics[\"precision\"],\n",
    "    \"g-mean\": metrics[\"g-mean\"]\n",
    "    }\n",
    "    \n",
    "    if ith == 0:\n",
    "        df_metrics_for_paper = pd.DataFrame(data=metrics_for_paper, index=[kfold_clf_names[ith].lower()])\n",
    "    else:\n",
    "        df_metrics_for_paper = df_metrics_for_paper.append(pd.Series(metrics_for_paper, name=kfold_clf_names[ith].lower()))\n",
    "    log.debug(df_metrics_for_paper)\n",
    "\n",
    "with open(\"capacity_metrics_mordred.tex\", \"w\") as fout:\n",
    "    cap = \"Classifier metrics for balanced data for capacity with models built from mordred features. MCC is the Matthew’s correlation coefficent.\"\n",
    "    df_metrics_for_paper.to_latex(fout, float_format=\"{:0.2f}\".format, position=\"H\", caption=cap, label=\"tbl:mordred_features\")\n",
    "log.info(df_metrics_for_paper.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fimp = pd.read_csv(\"importance_lr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fimp_mean = fimp.mean(axis=0)\n",
    "means = fimp_mean.values\n",
    "fimp_means = pd.DataFrame(means).transpose()\n",
    "fimp_means.columns=feats_df.columns\n",
    "\n",
    "fimp_sigma = fimp.std(axis=0)\n",
    "sigmas = fimp_sigma.values\n",
    "fimp_sigmas = pd.DataFrame(sigmas).transpose()\n",
    "fimp_sigmas.columns=feats_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(\" \".join(ent.split(\"_\"))) for ent in fimp_means.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(25,20))\n",
    "plt.bar(x=[\" \".join(ent.split(\"_\")) for ent in fimp_means.columns], \n",
    "        height=fimp_means.iloc[0,:].values, \n",
    "        width=1.0,\n",
    "        edgecolor=\"k\",\n",
    "        align=\"edge\")\n",
    "bins = np.arange(len(fimp_means.columns))\n",
    "plt.xlim([0,bins.size])\n",
    "plt.xlabel(\"Feature\", fontsize=35)\n",
    "plt.ylabel(\"Mean Coefficent value\", fontsize=35)\n",
    "plt.xticks(rotation=90, fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(\"Logistic Regression Mean Feature Importance using Mordred fingerprints\", fontsize=35)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance_lr_mordred.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
